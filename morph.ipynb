{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b845f44-457a-4738-a852-6badb8ce2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erbill/jupyter/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusers import AutoencoderKL\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.ema import calculate_posthoc_ema\n",
    "from utils import get_model, CLS_LOC_MAPPING\n",
    "from diffusion import create_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80699dbe-21d0-4fdb-ac86-70c64f17c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"/home/erbill/models/mp\"\n",
    "ema_std = 0.1\n",
    "cfg_scale = 4.0\n",
    "num_sampling_steps = 250\n",
    "seed = 42 #or None\n",
    "\n",
    "class_labels = [17, 17, 947, 947]\n",
    "\n",
    "num_samples = 4\n",
    "num_images = 512\n",
    "\n",
    "n_col = 2\n",
    "\n",
    "assert num_images % 64 == 0\n",
    "assert len(class_labels) % n_col == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2edf01-d607-4ea9-947e-74e75dfb90d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      " 24%|██▍       | 61/250 [00:30<01:35,  1.98it/s]"
     ]
    }
   ],
   "source": [
    "if seed:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(os.path.join(result_dir, \"config.yaml\"), \"r\") as f:\n",
    "    train_args = yaml.safe_load(f)\n",
    "\n",
    "# Load model\n",
    "model = get_model(train_args).to(device)\n",
    "\n",
    "# Load EMA state_dict\n",
    "state_dict = calculate_posthoc_ema(ema_std, os.path.join(result_dir, \"ema\"), verbose=True)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "#Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "\n",
    "out = []\n",
    "for class_label in class_labels:\n",
    "    z = torch.randn(num_samples, train_args[\"in_channels\"], train_args[\"input_size\"], train_args[\"input_size\"], device=device)\n",
    "    \n",
    "    res = []\n",
    "    interp_steps = num_images // num_samples\n",
    "    for idx_s in range(num_samples): \n",
    "        for t in range(interp_steps):\n",
    "            res.append(z[idx_s].lerp(z[(idx_s+1)%num_samples], t/interp_steps))\n",
    "            \n",
    "    z = torch.stack(res)\n",
    "    \n",
    "    # Labels to condition the model on\n",
    "    y = torch.tensor([class_label] * num_images, device=device)\n",
    "    \n",
    "    # Setup CFG\n",
    "    z = torch.cat([z, z], dim=0)\n",
    "    y_null = torch.tensor([1000] * num_images, device=device)\n",
    "    y = torch.cat([y, y_null], dim=0)\n",
    "    model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "    \n",
    "    # Sample images\n",
    "    diffusion = create_diffusion(str(num_sampling_steps))\n",
    "    samples = diffusion.ddim_sample_loop(\n",
    "        model.forward_with_cfg,\n",
    "        z.shape,\n",
    "        z,\n",
    "        clip_denoised=False,\n",
    "        model_kwargs=model_kwargs,\n",
    "        progress=True,\n",
    "        device=device,\n",
    "    )\n",
    "    # Remove null class samples\n",
    "    samples, _ = samples.chunk(2, dim=0)\n",
    "    \n",
    "    \n",
    "    # Denormalize samples\n",
    "    mean = torch.tensor(train_args[\"stats_mean\"]).reshape(1, -1, 1, 1).to(device)\n",
    "    std = torch.tensor(train_args[\"stats_std\"]).reshape(1, -1, 1, 1).to(device)\n",
    "    samples = samples * std + mean\n",
    "    \n",
    "    res = []\n",
    "    for idx in tqdm(range(0, num_images, 64)):\n",
    "        res.append(vae.decode(samples[idx:idx+64]).sample.cpu())\n",
    "    \n",
    "    samples = torch.cat(res, dim=0)\n",
    "    samples = samples.clamp(-1, 1)\n",
    "\n",
    "    out.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4ebed-59ae-4485-9cfb-91bb8d8d251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Concat to a grid of size (n//n_col) * n_col\n",
    "samples = torch.cat([torch.cat(out[i:i+n_col], dim=-1) for i in range(0, len(out), n_col)], dim=-2)\n",
    "images = [Image.fromarray(((img + 1) / 2 * 255).astype(np.uint8)) for img in samples.permute(0, 2, 3, 1).numpy()]\n",
    "\n",
    "images[0].save(\n",
    "    'output.gif',\n",
    "    save_all=True,\n",
    "    append_images=images[1:],  # Add the rest of the frames\n",
    "    duration=50,  # Duration between frames in milliseconds\n",
    "    loop=0  # Loop count, 0 means infinite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7df0e1-ec27-48be-9406-ee031a3942c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
